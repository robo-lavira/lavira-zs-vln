<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="LaViRA: a simple yet effective zero-shot Vision-and-Language Navigation framework accepted at ICRA 2026">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="LaViRA, Multimodal Large Language Models, Vision-and-Language Navigation, Zero-Shot Learning, Robotics, AI, Computer Vision">
  <!-- TODO: List all authors -->
  <meta name="author" content="Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang, Yang Gao">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Favicon -->
  <link rel="icon" href="static/images/favicon.ico" type="image/x-icon">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="LaViRA: a simple yet effective zero-shot Vision-and-Language Navigation framework accepted at ICRA 2026">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://robo-lavira.github.io/lavira-zs-vln/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://robo-lavira.github.io/lavira-zs-vln/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="LaViRA - Research Preview">
  <meta property="article:published_time" content="2025-10-22T00:00:00.000Z">
  <meta property="article:author" content="Hongyu Ding">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="LaViRA">
  <meta property="article:tag" content="Vision-and-Language Navigation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="LaViRA: a simple yet effective zero-shot Vision-and-Language Navigation framework accepted at ICRA 2026">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://robo-lavira.github.io/lavira-zs-vln/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="LaViRA - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments">
  <meta name="citation_author" content="Ding, Hongyu">
  <meta name="citation_author" content="Xu, Ziming">
  <meta name="citation_author" content="Fang, Yudong">
  <meta name="citation_author" content="Wu, You">
  <meta name="citation_author" content="Chen, Zixuan">
  <meta name="citation_author" content="Shi, Jieqi">
  <meta name="citation_author" content="Huo, Jing">
  <meta name="citation_author" content="Zhang, Yifan">
  <meta name="citation_author" content="Gao, Yang">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="ICRA 2026">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2510.19655">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</title>

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments",
    "description": "A simple yet effective zero-shot framework that decomposes navigation into Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation",
    "author": [
      {
        "@type": "Person",
        "name": "Hongyu Ding",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Computer Science, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Ziming Xu",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Computer Science, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Yudong Fang",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Intelligence Science and Technology, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "You Wu",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Computer Science, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Zixuan Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Computer Science, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Jieqi Shi",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Intelligence Science and Technology, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Jing Huo",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Computer Science, Nanjing University"
        }
      },
      {
        "@type": "Person",
        "name": "Yifan Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "Institute of Automation, Chinese Academy of Sciences"
        }
      },
      {
        "@type": "Person",
        "name": "Yang Gao",
        "affiliation": {
          "@type": "Organization",
          "name": "School of Intelligence Science and Technology, Nanjing University"
        }
      }
    ],
    "datePublished": "2025-10-22",
    "publisher": {
      "@type": "Organization",
      "name": "ICRA 2026"
    },
    "url": "https://robo-lavira.github.io/lavira-zs-vln/",
    "image": "https://robo-lavira.github.io/lavira-zs-vln/static/images/social_preview.png",
    "keywords": ["LaViRA", "Vision-Language Navigation", "Zero-Shot Learning", "Multimodal Large Language Models", "Robotics", "Computer Vision"],
    "abstract": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.",
    "citation": "@inproceedings{ding2025lavira, title={LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments}, author={Ding, Hongyu and Xu, Ziming and Fang, Yudong and Wu, You and Chen, Zixuan and Shi, Jieqi and Huo, Jing and Zhang, Yifan and Gao, Yang}, booktitle={ICRA 2026}, year={2026}}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://robo-lavira.github.io/lavira-zs-vln/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-Language Navigation"
      },
      {
        "@type": "Thing", 
        "name": "Robotics"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Nanjing University Robotics Lab",
    "url": "https://www.nju.edu.cn",
    "sameAs": [
      "https://github.com/robo-lavira"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">LaViRA: <u>La</u>nguage-<u>Vi</u>sion-<u>R</u>obot <u>A</u>ctions Translation for Zero-Shot Vision Language Navigation in Continuous Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                Hongyu Ding<sup>1,*</sup>,
              </span>
              <span class="author-block">
                Ziming Xu<sup>1,*</sup>,
              </span>
              <span class="author-block">
                Yudong Fang<sup>2</sup>,
              </span>
              <span class="author-block">
                You Wu<sup>1</sup>,
              </span>
              <span class="author-block">
                Zixuan Chen<sup>1</sup>,
              </span>
              <span class="author-block">
                Jieqi Shi<sup>2,â€ </sup>,
              </span>
              <span class="author-block">
                Jing Huo<sup>1,â€ </sup>,
              </span>
              <span class="author-block">
                Yifan Zhang<sup>3</sup>,
              </span>
              <span class="author-block">
                Yang Gao<sup>2</sup>
              </span>
            </div>

            <div class="is-size-7 publication-authors" style="margin-top: 10px;">
              <span class="author-block">
                <sup>1</sup>School of Computer Science, Nanjing University
              </span>
              <span class="author-block">
                <sup>2</sup>School of Intelligence Science and Technology, Nanjing University
              </span>
              <span class="author-block">
                <sup>3</sup>Institute of Automation, Chinese Academy of Sciences
              </span>
            </div>

            <div class="is-size-7 publication-authors" style="margin-top: 10px; color: #666;">
              <span class="author-block">
                <sup>*</sup>Equal Contribution, <sup>â€ </sup>Corresponding Author
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.19655" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span>
                <!-- TODO: Replace with your GitHub repository URL -->
                <span class="link-block">
                  <a href="https://github.com/robo-lavira/lavira-zs-vln" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="publication-meta" style="text-align: center; padding: 20px; background-color: #f5f5f5; margin: 20px 0;">
    <p style="margin: 0; color: #27ae60; font-weight: bold;">
      ðŸŽ‰ Accepted at ICRA 2026
    </p>
  </div>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation.
This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control.
LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments,  while maintaining transparency and efficiency for real-world deployment.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io/">Nerfies</a> project page.
              You are free to borrow <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">the source code</a> of this website, we just ask that you link back to this page in the footer.
              <br> <br>
              This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
